{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6de8b3e6",
   "metadata": {},
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ad4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    " \n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29756f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc31417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "random_seed=1234\n",
    "pl.seed_everything(random_seed)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a156867",
   "metadata": {},
   "source": [
    "%%time\n",
    "train_file = 'amex/agg_v3/agg_train_all_small.parquet'\n",
    "df=pd.read_parquet(train_file, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe48d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = ['B_10__first', 'B_10__last', 'B_10__max', 'B_10__mean', 'B_10__min', 'B_11__last', 'B_11__last2max', 'B_11__last__log', 'B_11__max', 'B_11__mean', 'B_11__mean__log', 'B_11__min', 'B_14__last2max', 'B_16__last', 'B_16__last2max', 'B_16__last__log', 'B_16__max', 'B_16__mean', 'B_16__mean__log', 'B_16__min', 'B_17__min', 'B_18__first', 'B_18__last', 'B_18__last2max', 'B_18__last__log', 'B_18__max', 'B_18__mean', 'B_18__mean__log', 'B_18__min', 'B_19__last', 'B_19__last__log', 'B_19__max', 'B_19__mean', 'B_19__mean__log', 'B_19__range', 'B_19__std', 'B_1__last', 'B_1__last2max', 'B_1__max', 'B_1__mean', 'B_1__min', 'B_20__last', 'B_20__last2max', 'B_20__last__log', 'B_20__max', 'B_20__mean', 'B_20__mean__log', 'B_20__range', 'B_22__last', 'B_22__last__log', 'B_22__max', 'B_22__mean', 'B_22__mean__log', 'B_22__range', 'B_22__std', 'B_23__first', 'B_23__last', 'B_23__last2max', 'B_23__last__log', 'B_23__max', 'B_23__mean', 'B_23__mean__log', 'B_23__min', 'B_23__range', 'B_23__std', 'B_25__last2max', 'B_26__last__log', 'B_28__last', 'B_28__last2max', 'B_2__first', 'B_2__last', 'B_2__last2max', 'B_2__last__log', 'B_2__max', 'B_2__mean', 'B_2__mean__log', 'B_2__min', 'B_30=0.0', 'B_30=1.0', 'B_30__nunique', 'B_33__first', 'B_33__last', 'B_33__last2max', 'B_33__last__log', 'B_33__max', 'B_33__mean', 'B_33__mean__log', 'B_33__min', 'B_37__last', 'B_37__last2max', 'B_37__max', 'B_37__mean', 'B_37__min', 'B_38=2.0', 'B_38=4.0', 'B_38=5.0', 'B_38__last', 'B_38__nunique', 'B_3__last', 'B_3__last2max', 'B_3__last__log', 'B_3__max', 'B_3__mean', 'B_3__mean__log', 'B_3__min', 'B_3__range', 'B_3__std', 'B_40__last', 'B_40__last__log', 'B_40__mean', 'B_40__mean__log', 'B_40__min', 'B_4__last', 'B_4__last2max', 'B_4__last__log', 'B_4__max', 'B_4__mean', 'B_4__mean__log', 'B_4__min', 'B_5__last__log', 'B_6__last', 'B_6__last2max', 'B_6__max', 'B_6__mean', 'B_6__min', 'B_7__first', 'B_7__last', 'B_7__last2max', 'B_7__max', 'B_7__mean', 'B_7__min', 'B_7__range', 'B_7__std', 'B_8__first', 'B_8__last', 'B_8__last__log', 'B_8__mean', 'B_8__mean__log', 'B_8__min', 'B_9__first', 'B_9__last', 'B_9__last2max', 'B_9__last__log', 'B_9__max', 'B_9__mean', 'B_9__mean__log', 'B_9__min', 'D_112__last', 'D_112__last2max', 'D_112__last__log', 'D_39__last', 'D_39__last__log', 'D_39__max', 'D_39__range', 'D_39__std', 'D_41__last', 'D_41__last__log', 'D_41__max', 'D_41__mean__log', 'D_41__range', 'D_41__std', 'D_42__first', 'D_42__last', 'D_42__max', 'D_42__mean', 'D_42__min', 'D_43__last', 'D_43__max', 'D_43__mean', 'D_43__mean__log', 'D_44__first', 'D_44__last', 'D_44__last2max', 'D_44__last__log', 'D_44__max', 'D_44__mean', 'D_44__mean__log', 'D_44__min', 'D_44__range', 'D_44__std', 'D_45__first', 'D_45__last', 'D_45__last__log', 'D_45__max', 'D_45__mean', 'D_45__mean__log', 'D_45__min', 'D_48__first', 'D_48__last', 'D_48__last2max', 'D_48__max', 'D_48__mean', 'D_48__min', 'D_51__mean__log', 'D_52__first', 'D_52__last', 'D_52__max', 'D_52__mean', 'D_52__min', 'D_53__last__log', 'D_53__max', 'D_53__mean__log', 'D_55__last', 'D_55__last__log', 'D_55__max', 'D_55__mean', 'D_55__mean__log', 'D_55__min', 'D_55__range', 'D_55__std', 'D_58__first', 'D_58__last', 'D_58__last2max', 'D_58__last__log', 'D_58__max', 'D_58__mean', 'D_58__mean__log', 'D_58__min', 'D_58__range', 'D_58__std', 'D_61__first', 'D_61__last', 'D_61__max', 'D_61__mean', 'D_61__min', 'D_62__first', 'D_62__last', 'D_62__max', 'D_62__mean', 'D_62__min', 'D_65__last__log', 'D_65__mean__log', 'D_70__last', 'D_70__last__log', 'D_70__max', 'D_70__mean', 'D_70__mean__log', 'D_70__range', 'D_70__std', 'D_74__first', 'D_74__last', 'D_74__last2max', 'D_74__last__log', 'D_74__max', 'D_74__mean', 'D_74__mean__log', 'D_74__min', 'D_74__range', 'D_74__std', 'D_75__first', 'D_75__last', 'D_75__last2max', 'D_75__last__log', 'D_75__max', 'D_75__mean', 'D_75__mean__log', 'D_75__min', 'D_75__range', 'D_75__std', 'D_77__first', 'D_77__last', 'D_77__last__log', 'D_77__max', 'D_77__mean', 'D_77__mean__log', 'D_77__min', 'D_78__last', 'D_78__last__log', 'D_78__max', 'D_78__mean', 'D_78__mean__log', 'D_78__range', 'D_78__std', 'D_84__max', 'D_84__mean', 'D_84__mean__log', 'D_84__range', 'D_84__std', 'P_2__first', 'P_2__last', 'P_2__last2max', 'P_2__max', 'P_2__mean', 'P_2__min', 'P_2__range', 'P_2__std', 'P_3__last', 'P_3__mean', 'P_3__min', 'R_10__max', 'R_10__mean', 'R_10__mean__log', 'R_10__range', 'R_10__std', 'R_15__max', 'R_15__mean__log', 'R_15__range', 'R_15__std', 'R_16__mean__log', 'R_1__last', 'R_1__last__log', 'R_1__max', 'R_1__mean', 'R_1__mean__log', 'R_1__range', 'R_1__std', 'R_2__last', 'R_2__last__log', 'R_2__max', 'R_2__mean', 'R_2__mean__log', 'R_2__range', 'R_2__std', 'R_3__last__log', 'R_3__max', 'R_3__mean', 'R_3__mean__log', 'R_3__min', 'R_3__range', 'R_3__std', 'R_4__last', 'R_4__max', 'R_4__mean', 'R_4__mean__log', 'R_4__range', 'R_4__std', 'R_5__last', 'R_5__max', 'R_5__mean', 'R_5__mean__log', 'R_5__range', 'R_5__std', 'R_6__last__log', 'R_6__max', 'R_6__mean', 'R_6__mean__log', 'R_6__range', 'R_6__std', 'R_7__mean', 'R_7__mean__log', 'R_7__range', 'R_7__std', 'R_8__max', 'R_8__mean', 'R_8__mean__log', 'R_8__range', 'R_8__std', 'S_15__max', 'S_15__mean', 'S_15__range', 'S_22__last', 'S_23__last', 'S_23__range', 'S_23__std', 'S_25__last2max', 'S_25__mean', 'S_25__min', 'S_25__range', 'S_25__std', 'S_3__last', 'S_3__max', 'S_3__mean', 'S_3__min', 'S_3__range', 'S_3__std', 'S_7__last', 'S_7__max', 'S_7__mean', 'S_7__range', 'S_7__std', 'S_8__last', 'S_8__mean', 'S_8__min']\n",
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d98821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "id_feats = ['customer_ID']\n",
    "#date_col =  'S_2'\n",
    "target_col = 'target'\n",
    "\n",
    "feats = ['B_10__last', 'B_10__mean', 'B_11__last', 'B_11__last2max', 'B_11__last__log', 'B_11__mean', 'B_11__mean__log', 'B_11__min', 'B_12__last', 'B_12__last__log', 'B_13__last', 'B_13__last__log', 'B_14__last', 'B_14__last2max', 'B_14__min', 'B_15__last', 'B_15__mean', 'B_16__last', 'B_16__max', 'B_16__min', 'B_17__last', 'B_17__max', 'B_17__mean', 'B_17__mean2std', 'B_17__min', 'B_18__last', 'B_18__last__log', 'B_18__mean', 'B_18__mean__log', 'B_18__min', 'B_19__last', 'B_19__last__log', 'B_19__min', 'B_1__last', 'B_1__last2max', 'B_1__max', 'B_1__mean', 'B_1__min', 'B_20__last', 'B_20__last__log', 'B_20__max', 'B_20__mean', 'B_20__mean__log', 'B_20__min', 'B_21__last', 'B_21__last__log', 'B_21__max', 'B_21__mean__log', 'B_22__last', 'B_22__last__log', 'B_22__max', 'B_22__mean', 'B_22__mean__log', 'B_22__min', 'B_23__last', 'B_23__last__log', 'B_23__mean', 'B_23__min', 'B_24__last', 'B_24__last__log', 'B_24__mean', 'B_24__mean__log', 'B_25__last', 'B_25__last2max', 'B_25__min', 'B_26__last', 'B_26__last__log', 'B_26__mean__log', 'B_27__last', 'B_27__last__log', 'B_28__last', 'B_29__last', 'B_29__last__log', 'B_2__last', 'B_2__last2max', 'B_2__last__log', 'B_2__mean', 'B_2__mean__log', 'B_2__min', 'B_30=0.0', 'B_30=1.0', 'B_30=2.0', 'B_30__last', 'B_30__nunique', 'B_31__last', 'B_31__nunique', 'B_32__last', 'B_32__last__log', 'B_32__max', 'B_33__last', 'B_33__last2max', 'B_33__last__log', 'B_33__max', 'B_33__mean', 'B_33__mean__log', 'B_33__min', 'B_36__last', 'B_36__last__log', 'B_37__last', 'B_37__last2max', 'B_37__max', 'B_37__mean', 'B_38=4.0', 'B_38=5.0', 'B_38=6.0', 'B_38=7.0', 'B_38__last', 'B_38__nunique', 'B_39__last', 'B_3__last', 'B_3__last__log', 'B_3__max', 'B_3__mean', 'B_3__mean__log', 'B_3__min', 'B_3__std', 'B_40__last', 'B_40__last__log', 'B_40__mean__log', 'B_40__min', 'B_41__last', 'B_41__last__log', 'B_42__last', 'B_42__last__log', 'B_4__last', 'B_4__last2max', 'B_4__last__log', 'B_4__max', 'B_4__mean2std', 'B_4__mean__log', 'B_5__last', 'B_5__last__log', 'B_5__mean', 'B_6__last', 'B_6__min', 'B_7__last', 'B_7__max', 'B_7__mean', 'B_7__min', 'B_8__last', 'B_8__last__log', 'B_8__max', 'B_8__mean', 'B_8__mean__log', 'B_8__min', 'B_9__last', 'B_9__last2max', 'B_9__last__log', 'B_9__max', 'B_9__mean', 'B_9__mean__log', 'B_9__min', 'D_102__last', 'D_102__last__log', 'D_102__max', 'D_103__last', 'D_104__last', 'D_105__last', 'D_105__max', 'D_106__last', 'D_106__last__log', 'D_107__last', 'D_107__last__log', 'D_107__max', 'D_107__mean__log', 'D_108__last', 'D_108__last__log', 'D_109__last', 'D_109__last__log', 'D_109__mean', 'D_110__last', 'D_111__last', 'D_112__last', 'D_112__last__log', 'D_113__last', 'D_113__last__log', 'D_113__max', 'D_114=0.0', 'D_114__last', 'D_114__nunique', 'D_115__last', 'D_115__last__log', 'D_116__last', 'D_117__last', 'D_118__last', 'D_118__last__log', 'D_118__mean__log', 'D_119__last', 'D_119__last__log', 'D_120=1.0', 'D_120__last', 'D_120__nunique', 'D_121__last', 'D_122__last', 'D_122__max', 'D_122__min', 'D_123__last', 'D_123__last__log', 'D_123__mean', 'D_124__last', 'D_125__last', 'D_125__last__log', 'D_126__last', 'D_127__last', 'D_128__last', 'D_128__max', 'D_128__min', 'D_129__last', 'D_129__max', 'D_129__mean', 'D_130__last', 'D_130__max', 'D_131__last', 'D_131__last__log', 'D_131__max', 'D_131__min', 'D_132__last', 'D_132__mean2std', 'D_132__min', 'D_133__last', 'D_133__last__log', 'D_133__max', 'D_133__min', 'D_134__last', 'D_134__min', 'D_135__last', 'D_135__last__log', 'D_135__mean2std', 'D_136__last', 'D_136__last__log', 'D_137__last', 'D_137__last__log', 'D_138__last', 'D_138__last__log', 'D_139__last', 'D_139__mean', 'D_140__last', 'D_140__last__log', 'D_140__max', 'D_141__last', 'D_142__last', 'D_143__last', 'D_144__last', 'D_145__last', 'D_39__last', 'D_39__last__log', 'D_39__max', 'D_39__mean', 'D_39__range', 'D_39__std', 'D_41__last', 'D_41__last__log', 'D_41__max', 'D_41__mean', 'D_41__min', 'D_42__first', 'D_42__last', 'D_42__max', 'D_42__mean', 'D_42__mean2std', 'D_42__min', 'D_43__last', 'D_43__last__log', 'D_43__max', 'D_43__mean', 'D_43__mean__log', 'D_43__min', 'D_44__last', 'D_44__last2max', 'D_44__last__log', 'D_44__max', 'D_44__mean', 'D_44__mean__log', 'D_44__min', 'D_44__range', 'D_44__std', 'D_45__first', 'D_45__last', 'D_45__last__log', 'D_45__max', 'D_45__mean', 'D_45__mean__log', 'D_45__min', 'D_46__last', 'D_46__mean', 'D_46__mean2std', 'D_46__min', 'D_47__last', 'D_48__first', 'D_48__last', 'D_48__max', 'D_48__mean', 'D_48__min', 'D_49__last', 'D_49__last__log', 'D_49__mean2std', 'D_50__last', 'D_51__last', 'D_51__last__log', 'D_51__mean', 'D_52__last', 'D_52__max', 'D_52__mean', 'D_52__mean2std', 'D_52__min', 'D_53__last', 'D_53__last__log', 'D_53__max', 'D_53__mean2std', 'D_53__min', 'D_54__last', 'D_55__last', 'D_55__last__log', 'D_55__max', 'D_55__min', 'D_56__last', 'D_56__min', 'D_58__last', 'D_58__min', 'D_59__last', 'D_59__max', 'D_60__last', 'D_60__last__log', 'D_60__max', 'D_61__last', 'D_61__max', 'D_61__mean', 'D_61__min', 'D_62__last', 'D_62__max', 'D_62__mean', 'D_62__min', 'D_64=U', 'D_65__last', 'D_65__last__log', 'D_65__max', 'D_65__mean', 'D_65__mean__log', 'D_68=1.0', 'D_68__last', 'D_69__last', 'D_69__min', 'D_70__last', 'D_70__max', 'D_70__mean2std', 'D_70__min', 'D_71__last', 'D_72__last', 'D_72__max', 'D_72__min', 'D_73__last', 'D_74__last', 'D_74__max', 'D_74__mean', 'D_75__last', 'D_75__last__log', 'D_75__max', 'D_75__mean', 'D_75__mean__log', 'D_76__last', 'D_77__last', 'D_77__max', 'D_77__mean', 'D_77__min', 'D_78__last', 'D_78__max', 'D_78__mean', 'D_78__mean__log', 'D_79__last', 'D_79__max', 'D_80__last', 'D_81__last', 'D_81__max', 'D_81__mean', 'D_82__last', 'D_83__last', 'D_84__last', 'D_84__max', 'D_84__mean', 'D_86__last', 'D_87__last', 'D_88__last', 'D_89__last', 'D_89__max', 'D_89__mean', 'D_91__last', 'D_92__last', 'D_93__last', 'D_94__last', 'D_96__last', 'P_2__first', 'P_2__last', 'P_2__last2max', 'P_2__max', 'P_2__mean', 'P_2__min', 'P_3__last', 'P_3__max', 'P_3__mean', 'P_3__min', 'P_4__last', 'P_4__max', 'P_4__mean', 'P_4__min', 'R_10__last', 'R_10__max', 'R_10__mean', 'R_10__mean__log', 'R_10__std', 'R_11__last', 'R_11__max', 'R_11__mean', 'R_12__last', 'R_12__max', 'R_12__mean', 'R_13__last', 'R_13__max', 'R_13__mean', 'R_13__mean2std', 'R_14__last', 'R_15__last', 'R_15__max', 'R_15__mean', 'R_16__last', 'R_16__max', 'R_16__mean', 'R_16__mean__log', 'R_17__last', 'R_17__max', 'R_18__last', 'R_18__max', 'R_19__last', 'R_19__min', 'R_1__last', 'R_1__last__log', 'R_1__max', 'R_1__mean', 'R_1__mean__log', 'R_1__min', 'R_1__range', 'R_1__std', 'R_20__last', 'R_20__max', 'R_20__mean', 'R_21__last', 'R_22__last', 'R_22__max', 'R_23__last', 'R_24__last', 'R_24__max', 'R_24__mean', 'R_25__last', 'R_25__max', 'R_26__last', 'R_26__mean2std', 'R_27__last', 'R_27__max', 'R_27__mean', 'R_27__min', 'R_28__last', 'R_28__last__log', 'R_2__last', 'R_2__last__log', 'R_2__max', 'R_2__mean', 'R_2__mean__log', 'R_2__min', 'R_2__range', 'R_2__std', 'R_3__last', 'R_3__last__log', 'R_3__max', 'R_3__mean', 'R_3__mean__log', 'R_3__min', 'R_4__last', 'R_4__max', 'R_4__mean', 'R_4__mean__log', 'R_4__min', 'R_5__last', 'R_5__max', 'R_5__mean', 'R_5__mean__log', 'R_5__range', 'R_6__last', 'R_6__last__log', 'R_6__max', 'R_6__mean', 'R_6__mean__log', 'R_7__last', 'R_7__max', 'R_7__mean', 'R_7__range', 'R_8__last', 'R_8__max', 'R_8__mean', 'R_8__mean__log', 'R_9__last', 'R_9__mean2std', 'S_11__last', 'S_11__mean', 'S_12__last', 'S_12__max', 'S_13__last', 'S_15__last', 'S_15__max', 'S_15__mean', 'S_16__last', 'S_17__last', 'S_17__min', 'S_18__last', 'S_19__last', 'S_20__last', 'S_20__max', 'S_22__last', 'S_22__max', 'S_22__mean', 'S_23__last', 'S_23__max', 'S_23__mean', 'S_24__last', 'S_25__last', 'S_25__last2max', 'S_25__mean', 'S_25__min', 'S_25__range', 'S_25__std', 'S_26__last', 'S_26__last__log', 'S_27__last', 'S_27__mean2std', 'S_3__last', 'S_3__max', 'S_3__mean', 'S_3__min', 'S_5__last', 'S_5__last__log', 'S_5__mean__log', 'S_6__last', 'S_6__max', 'S_6__min', 'S_7__last', 'S_7__max', 'S_7__mean', 'S_7__min', 'S_8__last', 'S_8__mean', 'S_8__min', 'S_9__last', 'S_9__max']\n",
    "\n",
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6ab5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_file = r'/kaggle/input/amex-agg-data-rev2/agg_train_all_rev2_rev.parquet'\n",
    "train_file = r'amex/agg_v3/agg_train_all_small.parquet'\n",
    "df = pd.read_parquet(train_file, columns=id_feats + [target_col] + feats, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39c855",
   "metadata": {},
   "source": [
    "## define 1d-cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a6d7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/scaomath/g2net-1d-cnn-gem-pool-pytorch-train-inference\n",
    "#Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "#https://github.com/baosenguo/Kaggle-MoA-2nd-Place-Solution\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CNN1d(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, #number of input features\n",
    "        output_size, #output dimension\n",
    "        hidden_sizes = [4096], #first layer linear output size\n",
    "        channels = [256, 512, 512], #channel sizes\n",
    "        dropouts = [0.1, 0.1, 0.1, 0.3, 0.2, 0.2], #dropout rates\n",
    "        celu_alpha = 0.06, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        #the channel size after reshaping linear output matrix\n",
    "        #for example, if Linear layer output is 4096 and first 1d CNN input channel is 256, \n",
    "        #then the reshape size is 16\n",
    "        linear_reshape_channel = int(hidden_sizes[0]/channels[0]) \n",
    "        avg_pool_output = int(hidden_sizes[0]/channels[0]/2)\n",
    "        max_pool_output = int(hidden_sizes[0]/channels[0]/2/2)*channels[2]\n",
    "        \n",
    "        self.linear_reshape_channel = linear_reshape_channel\n",
    "        self.channels = channels\n",
    "        \n",
    "        #transform n*m (m=input_size) matrix into n*hidden_sizes[0] matrix\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_size),\n",
    "            nn.Dropout(dropouts[0]),\n",
    "            nn.utils.weight_norm(nn.Linear(input_size, hidden_sizes[0])),\n",
    "            nn.CELU(alpha=celu_alpha)\n",
    "        )\n",
    "        \n",
    "        #1st layer of convolutional network\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channels[0]), \n",
    "            nn.Dropout(dropouts[1]),\n",
    "            nn.utils.weight_norm(nn.Conv1d(in_channels = channels[0], \n",
    "                                           out_channels = channels[1], \n",
    "                                           kernel_size = 5, \n",
    "                                           stride=1, \n",
    "                                           padding=2, \n",
    "                                           bias=False), dim=None),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size = avg_pool_output)\n",
    "\n",
    "        )\n",
    "        \n",
    "        #2nd layer of convolutional network\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channels[1]), \n",
    "            nn.Dropout(dropouts[2]),\n",
    "            nn.utils.weight_norm(nn.Conv1d(in_channels = channels[1], \n",
    "                                           out_channels = channels[1], \n",
    "                                           kernel_size = 3, \n",
    "                                           stride=1, \n",
    "                                           padding=1, \n",
    "                                           bias=True), dim=None),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        )\n",
    "        \n",
    "        #3rd layer of convolutional network\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channels[1]), \n",
    "            nn.Dropout(dropouts[3]),\n",
    "            nn.utils.weight_norm(nn.Conv1d(in_channels = channels[1], \n",
    "                                           out_channels = channels[1], \n",
    "                                           kernel_size = 3, \n",
    "                                           stride=1, \n",
    "                                           padding=1,  \n",
    "                                           bias=True), dim=None),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(channels[1]), \n",
    "            nn.Dropout(dropouts[4]),\n",
    "            nn.utils.weight_norm(nn.Conv1d(in_channels = channels[1], \n",
    "                                           out_channels = channels[2], \n",
    "                                           kernel_size = 5, \n",
    "                                           stride=1, \n",
    "                                           padding=2, \n",
    "                                           bias=True), dim=None),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        #output layer \n",
    "        self.out = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 4, stride=2, padding=1),\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(max_pool_output),\n",
    "            nn.Dropout(dropouts[5]),\n",
    "            nn.utils.weight_norm(nn.Linear(max_pool_output, output_size)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = x.reshape(x.shape[0],  self.channels[0], self.linear_reshape_channel)\n",
    "        \n",
    "        x = self.cnn1(x)\n",
    "        x_cnn2 = self.cnn2(x)\n",
    "        \n",
    "        x = self.cnn3(x)\n",
    "        x = x_cnn2*x\n",
    "        \n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff99e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (Dataset, DataLoader)\n",
    "  \n",
    "\n",
    "class TS_Data(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y): \n",
    "        \n",
    "        features = torch.FloatTensor(X)\n",
    "        targets = torch.FloatTensor(y)\n",
    "        \n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "        x = self.features[idx]\n",
    "        y = self.targets[idx]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "\n",
    "def load_data(X, y, batch_size, n_workers=0, shuffle=False):\n",
    "    data = TS_Data(X, y)\n",
    "    \n",
    "    loader = DataLoader(data, batch_size=batch_size, num_workers=n_workers, shuffle=shuffle)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd768f7",
   "metadata": {},
   "source": [
    "## hyperopt parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0208565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 5, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_rates = np.concatenate((np.arange(0.00001, 0.0001, 0.00001),  \n",
    "                           np.arange(0.0001, 0.001, 0.0001), \n",
    "                           np.arange(0.001, 0.01, 0.001), \n",
    "                           np.arange(0.01, 0.05, 0.01)\n",
    "                          ), \n",
    "                          axis=0)\n",
    "hidden_sizes=[256, 512, 1024, 2048, 4096] \n",
    "dropouts = np.round(np.arange(0.001, 0.501, 0.001), 4)\n",
    "channel_list = [16, 32, 64, 128, 256, 512]\n",
    "\n",
    "len(learn_rates), len(hidden_sizes), len(dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31809347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "space  = { \n",
    "             'batch_size': hp.choice('batch_size', [128*i for i in [1, 2, 4, 8, 16, 20, 32, 40, 80, 100]]),\n",
    "             'num_epochs':hp.choice('num_epochs', range(5, 65, 5)),\n",
    "             'learning_rate':hp.choice('learning_rate', learn_rates),\n",
    "             'hidden_size1':hp.choice('hidden_size1', hidden_sizes),\n",
    "             'dropout1':  hp.choice('dropout1', dropouts), \n",
    "             'dropout2':  hp.choice('dropout2', dropouts), \n",
    "             'dropout3':  hp.choice('dropout3', dropouts), \n",
    "             'dropout4':  hp.choice('dropout4', dropouts), \n",
    "             'dropout5':  hp.choice('dropout5', dropouts), \n",
    "             'dropout6':  hp.choice('dropout6', dropouts), \n",
    "             'celu_alpha': hp.choice('celu_alpha', [0.001, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.2]), \n",
    "             'channel':  hp.choice('channel', channel_list), \n",
    "            \n",
    "    \n",
    "            }                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791b7a7",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09854481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @yunchonggan's fast metric implementation\n",
    "# From https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "# https://www.kaggle.com/code/ambrosm/amex-lightgbm-quickstart\n",
    "def amex_metric(y_true: np.array, y_pred: np.array) -> float:\n",
    "\n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d431e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feats]\n",
    "y = df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12361eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dfae537",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8abd675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=None, shuffle=False)\n",
      "TRAIN: [152971 152972 152973 ... 458910 458911 458912] TEST: [     0      1      2 ... 152968 152969 152970]\n",
      "TRAIN: [     0      1      2 ... 458910 458911 458912] TEST: [152971 152972 152973 ... 305939 305940 305941]\n",
      "TRAIN: [     0      1      2 ... 305939 305940 305941] TEST: [305942 305943 305944 ... 458910 458911 458912]\n"
     ]
    }
   ],
   "source": [
    "print(skf)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbdb4bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "795eb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4648478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = []\n",
    "\n",
    "def score(params):\n",
    "    pl.seed_everything(1)\n",
    "\n",
    "    \n",
    "    if params['hidden_size1']<4*params['channel']:\n",
    "        print('invalid parameters')\n",
    "        return {'loss': 9999, 'status': STATUS_OK}\n",
    "    \n",
    "    print(params)\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    h_sizes = [params['hidden_size1']]\n",
    "    drop_list = [params[f'dropout{i}'] for i in range(1,7)]\n",
    "    celu_alpha = params['celu_alpha']\n",
    "    channels = [params['channel'], 2*params['channel'], 2*params['channel']]\n",
    "\n",
    "        \n",
    "    losses = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        #----start: data prep-------------------------------------\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        #----end: data prep-------------------------------------\n",
    "        \n",
    "#         print(X_train.shape, X_test.shape)\n",
    "        \n",
    "        \n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        minmax_scaler.fit(X_train)\n",
    "        # minmax_scaler.fit_transform(X_train[x_cols])\n",
    "\n",
    "        train_loader = load_data(minmax_scaler.transform(X_train), y_train['target'].values, \n",
    "                                 batch_size=batch_size, n_workers=0, shuffle=False)\n",
    "\n",
    "        test_loader = load_data(minmax_scaler.transform(X_test), y_test['target'].values, \n",
    "                                 batch_size=batch_size, n_workers=0, shuffle=False)\n",
    "        #----end: data prep-------------------------------------\n",
    "\n",
    "\n",
    "        model = CNN1d(input_size=len(feats), \n",
    "                      output_size=1, \n",
    "                      hidden_sizes = h_sizes, \n",
    "                      channels = channels, \n",
    "                      dropouts = drop_list, \n",
    "                      celu_alpha = celu_alpha)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        # optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)  \n",
    "        optimizer = torch.optim.RMSprop([\n",
    "                {'params': model.linear1.parameters()},\n",
    "                {'params': model.cnn1.parameters()},\n",
    "                {'params': model.cnn2.parameters()},\n",
    "                {'params': model.cnn3.parameters()},\n",
    "                {'params': model.out.parameters()},\n",
    "            ], lr=learning_rate)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                                  max_lr=1e-2, epochs=num_epochs, steps_per_epoch=len(train_loader))\n",
    "        out_loss = BCELoss()\n",
    "\n",
    "\n",
    "        #------train models--------------------------\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                ### FORWARD AND BACK PROP\n",
    "                out = model(features)\n",
    "                \n",
    "                out_cost = out_loss(out.squeeze(), targets) \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out_cost.backward()\n",
    "\n",
    "                ### UPDATE MODEL PARAMETERS\n",
    "                optimizer.step()\n",
    "\n",
    "        #-----eval models-------------------------------\n",
    "        model.eval()\n",
    "\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        with torch.no_grad():\n",
    "            for features, targets in test_loader:\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(features)\n",
    "                y_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "                y_trues.extend(targets.squeeze().cpu().numpy())\n",
    "  \n",
    "        y_preds = np.array(y_preds)\n",
    "        loss = amex_metric(np.array(y_trues), y_preds)\n",
    "        #loss = roc_auc_score(y_trues, y_preds)        \n",
    "        losses.append(loss)\n",
    "        \n",
    "    loss = np.mean(losses)\n",
    "    print(loss)\n",
    "    loss_dict.append({'params': params, 'losses': losses, 'mean_loss': loss})\n",
    "    \n",
    "    return {'loss': -loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f9c698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, anneal, rand\n",
    "from functools import partial\n",
    "def optimize(space, evals, cores, trials, optimizer=tpe.suggest, random_state=1234, n_startup_jobs=10):\n",
    "    algo = partial(optimizer, n_startup_jobs=n_startup_jobs)\n",
    "    best = fmin(score, space, algo=algo, max_evals=evals, trials = trials)\n",
    "    print(best)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f30fa640",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = 4\n",
    "n=500\n",
    "verbose = False\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7560ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = optimize(space,\n",
    "                      evals = n,\n",
    "                      optimizer=tpe.suggest,\n",
    "                      cores = cores,\n",
    "                      trials = trials, random_state=1234, \n",
    "                      n_startup_jobs=10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6ea312f",
   "metadata": {},
   "source": [
    "{'batch_size': 4096, 'celu_alpha': 0.01, 'channel': 16, 'dropout1': 0.195, 'dropout2': 0.479, 'dropout3': 0.343, 'dropout4': 0.055, 'dropout5': 0.181, 'dropout6': 0.27, 'hidden_size1': 1024, 'learning_rate': 0.0008, 'num_epochs': 100}\n",
    "0.7766768220546116 \n",
    "\n",
    "{'batch_size': 10240, 'celu_alpha': 0.05, 'channel': 64, 'dropout1': 0.307, 'dropout2': 0.406, 'dropout3': 0.201, 'dropout4': 0.444, 'dropout5': 0.41, 'dropout6': 0.084, 'hidden_size1': 1024, 'learning_rate': 0.006, 'num_epochs': 45}\n",
    "0.7863856178482568  \n",
    "\n",
    "{'batch_size': 4096, 'celu_alpha': 0.1, 'channel': 256, 'dropout1': 0.383, 'dropout2': 0.06, 'dropout3': 0.261, 'dropout4': 0.08, 'dropout5': 0.253, 'dropout6': 0.224, 'hidden_size1': 1024, 'learning_rate': 0.0004, 'num_epochs': 5}\n",
    "0.7825551430613326     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
