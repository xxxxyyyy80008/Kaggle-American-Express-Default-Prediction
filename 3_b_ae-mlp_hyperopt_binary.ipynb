{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ad4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    " \n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29756f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc31417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 8968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8968"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "random_seed=8968\n",
    "pl.seed_everything(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d16844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_file = r'amex\\agg_train_all_rev.parquet'\n",
    "df=pd.read_parquet(train_file, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae1933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cnt', 'B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3', 'B_32', 'B_33', 'B_36', 'B_37', 'B_39', 'B_4', 'B_40', 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'D_108', 'D_109', 'D_110', 'D_111', 'D_112', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_127', 'D_128', 'D_129', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_65', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_84', 'D_86', 'D_87', 'D_88', 'D_89', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96', 'P_2', 'P_3', 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_16', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', 'R_21', 'R_22', 'R_23', 'R_24', 'R_25', 'R_26', 'R_27', 'R_28', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'days']\n",
      "['log_B_11', 'log_B_12', 'log_B_13', 'log_B_21', 'log_B_22', 'log_B_23', 'log_B_24', 'log_B_26', 'log_B_27', 'log_B_28', 'log_B_29', 'log_B_3', 'log_B_32', 'log_B_36', 'log_B_4', 'log_B_40', 'log_B_41', 'log_B_42', 'log_B_5', 'log_B_9', 'log_D_106', 'log_D_107', 'log_D_108', 'log_D_109', 'log_D_113', 'log_D_115', 'log_D_118', 'log_D_119', 'log_D_123', 'log_D_125', 'log_D_131', 'log_D_133', 'log_D_135', 'log_D_136', 'log_D_137', 'log_D_138', 'log_D_140', 'log_D_39', 'log_D_41', 'log_D_43', 'log_D_44', 'log_D_45', 'log_D_49', 'log_D_51']\n"
     ]
    }
   ],
   "source": [
    "all_cols = ['cnt', 'B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3', 'B_30=0.0', 'B_30=1.0', 'B_30=2.0', 'B_31=0', 'B_31=1', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38=1.0', 'B_38=2.0', 'B_38=3.0', 'B_38=4.0', 'B_38=5.0', 'B_38=6.0', 'B_38=7.0', 'B_39', 'B_4', 'B_40', 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'D_108', 'D_109', 'D_110', 'D_111', 'D_112', 'D_113', 'D_114=0.0', 'D_114=1.0', 'D_115', 'D_116=0.0', 'D_116=1.0', 'D_117=-1.0', 'D_117=1.0', 'D_117=2.0', 'D_117=3.0', 'D_117=4.0', 'D_117=5.0', 'D_117=6.0', 'D_118', 'D_119', 'D_120=0.0', 'D_120=1.0', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126=-1.0', 'D_126=0.0', 'D_126=1.0', 'D_127', 'D_128', 'D_129', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63=CL', 'D_63=CO', 'D_63=CR', 'D_63=XL', 'D_63=XM', 'D_63=XZ', 'D_64=-1', 'D_64=O', 'D_64=R', 'D_64=U', 'D_65', 'D_66=0.0', 'D_66=1.0', 'D_68=0.0', 'D_68=1.0', 'D_68=2.0', 'D_68=3.0', 'D_68=4.0', 'D_68=5.0', 'D_68=6.0', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_84', 'D_86', 'D_87', 'D_88', 'D_89', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96', 'P_2', 'P_3', 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_16', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', 'R_21', 'R_22', 'R_23', 'R_24', 'R_25', 'R_26', 'R_27', 'R_28', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27', 'S_2=max', 'S_2=min', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'customer_ID', 'days']\n",
    "\n",
    "cat_feats = ['B_30=0.0', 'B_30=1.0', 'B_30=2.0', 'B_31=0', 'B_31=1', 'B_38=1.0', 'B_38=2.0', 'B_38=3.0', 'B_38=4.0', 'B_38=5.0', 'B_38=6.0', 'B_38=7.0', 'D_114=0.0', 'D_114=1.0', 'D_116=0.0', 'D_116=1.0', 'D_117=-1.0', 'D_117=1.0', 'D_117=2.0', 'D_117=3.0', 'D_117=4.0', 'D_117=5.0', 'D_117=6.0', 'D_120=0.0', 'D_120=1.0', 'D_126=-1.0', 'D_126=0.0', 'D_126=1.0', 'D_63=CL', 'D_63=CO', 'D_63=CR', 'D_63=XL', 'D_63=XM', 'D_63=XZ', 'D_64=-1', 'D_64=O', 'D_64=R', 'D_64=U', 'D_66=0.0', 'D_66=1.0', 'D_68=0.0', 'D_68=1.0', 'D_68=2.0', 'D_68=3.0', 'D_68=4.0', 'D_68=5.0', 'D_68=6.0']\n",
    "s2_feats = ['S_2=max', 'S_2=min']\n",
    "float_feats = ['cnt', 'B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3', 'B_32', 'B_33', 'B_36', 'B_37', 'B_39', 'B_4', 'B_40', 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'D_108', 'D_109', 'D_110', 'D_111', 'D_112', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_127', 'D_128', 'D_129', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_65', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_84', 'D_86', 'D_87', 'D_88', 'D_89', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96', 'P_2', 'P_3', 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_16', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', 'R_21', 'R_22', 'R_23', 'R_24', 'R_25', 'R_26', 'R_27', 'R_28', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'days']\n",
    "log_feats = ['log_B_11', 'log_B_12', 'log_B_13', 'log_B_21', 'log_B_22', 'log_B_23', 'log_B_24', 'log_B_26', 'log_B_27', 'log_B_28', 'log_B_29', 'log_B_3', 'log_B_32', 'log_B_36', 'log_B_4', 'log_B_40', 'log_B_41', 'log_B_42', 'log_B_5', 'log_B_9', 'log_D_106', 'log_D_107', 'log_D_108', 'log_D_109', 'log_D_113', 'log_D_115', 'log_D_118', 'log_D_119', 'log_D_123', 'log_D_125', 'log_D_131', 'log_D_133', 'log_D_135', 'log_D_136', 'log_D_137', 'log_D_138', 'log_D_140', 'log_D_39', 'log_D_41', 'log_D_43', 'log_D_44', 'log_D_45', 'log_D_49', 'log_D_51']\n",
    "\n",
    "print(float_feats)\n",
    "print(log_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39c855",
   "metadata": {},
   "source": [
    "## define autoencoder-mlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6d7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/mlp/submodules.html#FullyConnectedModule\n",
    "#https://www.kaggle.com/c/jane-street-market-prediction/discussion/224348\n",
    "#https://www.kaggle.com/code/gogo827jz/jane-street-supervised-autoencoder-mlp/notebook?scriptVersionId=73762661\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AE_MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_sizes: list,\n",
    "        dropouts: list,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropouts = dropouts\n",
    "        \n",
    "        #----normalize input data--------------\n",
    "        self.bn0 = nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        #---encoder layer----------------\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]), \n",
    "                                     nn.BatchNorm1d(hidden_sizes[0]), \n",
    "                                     nn.SiLU()\n",
    "                                    )\n",
    "        #---decoder layer----------------\n",
    "        self.decoder = nn.Sequential(nn.Dropout(dropouts[0]), \n",
    "                                     nn.Linear(hidden_sizes[0], input_size) \n",
    "                                    )\n",
    "        #----AE output layer-------------\n",
    "        self.ae_out = nn.Sequential(nn.Linear(input_size, hidden_sizes[1]), \n",
    "                                    nn.BatchNorm1d(hidden_sizes[1]), \n",
    "                                    nn.SiLU(), \n",
    "                                    nn.Dropout(dropouts[1]), \n",
    "                                    nn.Linear(hidden_sizes[1], output_size),\n",
    "                                    nn.Sigmoid(), #for binary classification loss function BCELoss\n",
    "                                    )\n",
    "        #---MLP--------------------------\n",
    "                \n",
    "        # input layer\n",
    "        size2 = input_size+hidden_sizes[0]\n",
    "        module_list = [nn.BatchNorm1d(size2), \n",
    "                       nn.Dropout(dropouts[2]), \n",
    "                       nn.Linear(size2, hidden_sizes[2]), \n",
    "                       nn.BatchNorm1d(hidden_sizes[2]), \n",
    "                       nn.SiLU(), \n",
    "                       nn.Dropout(dropouts[2])]\n",
    "    \n",
    "        # hidden layers\n",
    "        for i in range(3, len(hidden_sizes)):\n",
    "            module_list.extend([nn.Linear(hidden_sizes[i-1], hidden_sizes[i]), \n",
    "                                nn.BatchNorm1d(hidden_sizes[i]), \n",
    "                                nn.SiLU(), \n",
    "                                nn.Dropout(dropouts[i])]\n",
    "                              )\n",
    "        # output layer\n",
    "        module_list.extend([nn.Linear(hidden_sizes[-1], output_size), \n",
    "                           nn.Sigmoid()])\n",
    "\n",
    "        self.mlp = nn.Sequential(*module_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = self.bn0(x)\n",
    "        encoder = self.encoder(x0)\n",
    "        decoder = self.decoder(encoder)\n",
    "        out_ae = self.ae_out(decoder)\n",
    "        \n",
    "        #x0 shape is n*m - n samples, m features\n",
    "        #encoder is n*k - n samples, k features\n",
    "        #x1 is n*(k+m) - n samples, (k+m) features\n",
    "        #if x0 is n*w*m with w as the width for 3d array, the output will be n*w*(k+m)\n",
    "        x1 = torch.cat((x0, encoder), dim = -1) #\n",
    "        out = self.mlp(x1)\n",
    "\n",
    "        return decoder, out_ae, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff99e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (Dataset, DataLoader)\n",
    "  \n",
    "\n",
    "class TS_Data(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y): \n",
    "        \n",
    "        features = torch.FloatTensor(X)\n",
    "        targets = torch.FloatTensor(y)\n",
    "        \n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "        x = self.features[idx]\n",
    "        y = self.targets[idx]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "\n",
    "def load_data(X, y, batch_size, n_workers=0, shuffle=False):\n",
    "    data = TS_Data(X, y)\n",
    "    \n",
    "    loader = DataLoader(data, batch_size=batch_size, num_workers=n_workers, shuffle=shuffle)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd768f7",
   "metadata": {},
   "source": [
    "## hyperopt parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0208565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 10, 500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_rates = np.concatenate((np.arange(0.00001, 0.0001, 0.00001),  \n",
    "                           np.arange(0.0001, 0.001, 0.0001), \n",
    "                           np.arange(0.001, 0.01, 0.001), \n",
    "                           np.arange(0.01, 0.05, 0.01)\n",
    "                          ), \n",
    "                          axis=0)\n",
    "hidden_sizes=[32, 48, 64, 96, 128, 256, 448, 512, 896, 1024] \n",
    "dropouts = np.round(np.arange(0.001, 0.501, 0.001), 4)\n",
    "\n",
    "len(learn_rates), len(hidden_sizes), len(dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31809347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "space  = { \n",
    "             'batch_size': hp.choice('batch_size', [128*16, 128*20, 128*32, 128*40, 128*80,  128*120]),\n",
    "             'num_epochs':hp.choice('num_epochs', [50, 60, 100, 150, 200]),\n",
    "             'learning_rate':hp.choice('learning_rate', learn_rates),\n",
    "             'hidden_size1':hp.choice('hidden_size1', hidden_sizes),\n",
    "             'hidden_size2':hp.choice('hidden_size2', hidden_sizes),\n",
    "             'hidden_size3':hp.choice('hidden_size3', hidden_sizes),\n",
    "             'hidden_size4':hp.choice('hidden_size4', hidden_sizes),\n",
    "             'hidden_size5':hp.choice('hidden_size5', hidden_sizes),\n",
    "             'hidden_size6':hp.choice('hidden_size6', hidden_sizes),\n",
    "             'dropout1':  hp.choice('dropout1', dropouts), \n",
    "             'dropout2':  hp.choice('dropout2', dropouts), \n",
    "             'dropout3':  hp.choice('dropout3', dropouts), \n",
    "             'dropout4':  hp.choice('dropout4', dropouts), \n",
    "             'dropout5':  hp.choice('dropout5', dropouts), \n",
    "             'dropout6':  hp.choice('dropout6', dropouts), \n",
    "    \n",
    "            }                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791b7a7",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dada0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc45158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cols = float_feats + cat_feats + log_feats\n",
    "len(x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71888fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = ['P_2', 'B_20', 'R_1', 'D_46', 'log_B_26', 'log_B_9', 'B_33', 'log_B_23', 'D_39', 'S_3', 'D_43', 'B_18', 'log_B_22', 'R_8', 'log_D_51', 'R_4', 'B_9', 'B_1', 'B_10']\n",
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf761e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feats]\n",
    "y = df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad678346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 458913 entries, 0 to 458912\n",
      "Columns: 273 entries, customer_ID to target\n",
      "dtypes: float32(267), int32(2), int64(1), object(3)\n",
      "memory usage: 488.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98ef5f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49a4b48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "skf.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56ab481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
      "TRAIN: [     0      1      2 ... 458910 458911 458912] TEST: [     4      7     19 ... 458899 458902 458906]\n",
      "TRAIN: [     2      3      4 ... 458909 458910 458911] TEST: [     0      1      5 ... 458907 458908 458912]\n",
      "TRAIN: [     0      1      2 ... 458909 458910 458912] TEST: [    13     17     26 ... 458901 458903 458911]\n",
      "TRAIN: [     0      1      3 ... 458909 458911 458912] TEST: [     2     10     11 ... 458898 458905 458910]\n",
      "TRAIN: [     0      1      2 ... 458910 458911 458912] TEST: [     3      6     15 ... 458897 458904 458909]\n"
     ]
    }
   ],
   "source": [
    "print(skf)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbdb4bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "795eb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4648478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = []\n",
    "\n",
    "def score(params):\n",
    "    pl.seed_everything(1)\n",
    "    print(params)\n",
    "    \n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    h_sizes = [params[f'hidden_size{i}'] for i in range(1,7)]\n",
    "    drop_list = [params[f'dropout{i}'] for i in range(1,7)]\n",
    "\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        #----start: data prep-------------------------------------\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        #----end: data prep-------------------------------------\n",
    "        \n",
    "#         print(X_train.shape, X_test.shape)\n",
    "        \n",
    "        \n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        minmax_scaler.fit(X_train)\n",
    "        # minmax_scaler.fit_transform(X_train[x_cols])\n",
    "\n",
    "        train_loader = load_data(minmax_scaler.transform(X_train), y_train['target'].values, \n",
    "                                 batch_size=batch_size, n_workers=0, shuffle=False)\n",
    "\n",
    "        test_loader = load_data(minmax_scaler.transform(X_test), y_test['target'].values, \n",
    "                                 batch_size=batch_size, n_workers=0, shuffle=False)\n",
    "        #----end: data prep-------------------------------------\n",
    "\n",
    "\n",
    "        model = AE_MLP(input_size=len(feats), output_size=1, \n",
    "                       hidden_sizes=h_sizes, \n",
    "                       dropouts = drop_list)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        # optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)  \n",
    "        optimizer = torch.optim.RMSprop([\n",
    "                {'params': model.encoder.parameters()},\n",
    "                {'params': model.decoder.parameters()},\n",
    "                {'params': model.ae_out.parameters()},\n",
    "                {'params': model.mlp.parameters()},\n",
    "            ], lr=learning_rate)\n",
    "\n",
    "\n",
    "        decoder_loss = MSELoss()\n",
    "        out_ae_loss = BCELoss()\n",
    "        out_loss = BCELoss()\n",
    "\n",
    "\n",
    "        #------train models--------------------------\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                ### FORWARD AND BACK PROP\n",
    "                decoder, out_ae, out = model(features)\n",
    "\n",
    "                decoder_cost = decoder_loss(decoder, features)\n",
    "                out_ae_cost = out_ae_loss(out_ae.squeeze(), targets)  \n",
    "                out_cost = out_loss(out.squeeze(), targets) #squeeze the n_samples*1 2d array to 1d array of n_samples\n",
    "                total_cost = (decoder_cost + out_ae_cost + out_cost)/3\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                total_cost.backward()\n",
    "\n",
    "                ### UPDATE MODEL PARAMETERS\n",
    "                optimizer.step()\n",
    "\n",
    "        #-----eval models-------------------------------\n",
    "        model.eval()\n",
    "\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        with torch.no_grad():\n",
    "            for features, targets in test_loader:\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                _, _, outputs = model(features)\n",
    "                y_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "                y_trues.extend(targets.squeeze().cpu().numpy())\n",
    "\n",
    "        #-----start: train mlp---------------------------------------\n",
    "        \n",
    "        #-----end: train mlp---------------------------------------\n",
    "#         loss = roc_auc_score(y_trues, y_preds)\n",
    "        loss = amex_metric(y_test, \n",
    "                           pd.DataFrame(data={'prediction': y_preds}))\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        \n",
    "    loss = np.mean(losses)\n",
    "    print(loss)\n",
    "    loss_dict.append({'params': params, 'losses': losses, 'mean_loss': loss})\n",
    "    \n",
    "    return {'loss': -loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f9c698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, anneal, rand\n",
    "from functools import partial\n",
    "def optimize(space, evals, cores, trials, optimizer=tpe.suggest, random_state=1234, n_startup_jobs=10):\n",
    "    algo = partial(optimizer, n_startup_jobs=n_startup_jobs)\n",
    "    best = fmin(score, space, algo=algo, max_evals=evals, trials = trials)\n",
    "    print(best)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f30fa640",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = 4\n",
    "n=500\n",
    "verbose = False\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67716392",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = optimize(space,\n",
    "                      evals = n,\n",
    "                      optimizer=tpe.suggest,\n",
    "                      cores = cores,\n",
    "                      trials = trials, random_state=1234, \n",
    "                      n_startup_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462978a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
