{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read a large csv file with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-06T05:00:33.400180Z",
     "iopub.status.busy": "2022-07-06T05:00:33.399724Z",
     "iopub.status.idle": "2022-07-06T05:00:33.411079Z",
     "shell.execute_reply": "2022-07-06T05:00:33.409910Z",
     "shell.execute_reply.started": "2022-07-06T05:00:33.400144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/amex-default-prediction/sample_submission.csv\n",
      "/kaggle/input/amex-default-prediction/train_data.csv\n",
      "/kaggle/input/amex-default-prediction/test_data.csv\n",
      "/kaggle/input/amex-default-prediction/train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### references:\n",
    "\n",
    "1. read file by chunksize\n",
    " - https://stackoverflow.com/questions/25962114/how-do-i-read-a-large-csv-file-with-pandas\n",
    " - https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    "1. convert 64bit numeric values to 32bit values: convert int64 to int32; convert float64 to float32\n",
    " - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html\n",
    " - **warning**: if the values is out of 32bit value range, the conversion will be erroneous\n",
    "1. save data to parquet file: use compression='GZIP' to further decrease file size\n",
    " - https://arrow.apache.org/docs/python/parquet.html\n",
    " - https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html?highlight=write_table\n",
    "\n",
    "#### steps:\n",
    "\n",
    "1. read csv by chunk. can set chunk size as 100k (i.e. chunksize = 10e4)\n",
    "1. convert int64 to int32 and float64 to float32 >> this will cut the file size to half\n",
    "1. save each chunk to a parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T05:05:58.376354Z",
     "iopub.status.busy": "2022-07-06T05:05:58.375811Z",
     "iopub.status.idle": "2022-07-06T05:05:58.384737Z",
     "shell.execute_reply": "2022-07-06T05:05:58.383276Z",
     "shell.execute_reply.started": "2022-07-06T05:05:58.376309Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from dateutil import relativedelta\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T04:26:22.452366Z",
     "iopub.status.busy": "2022-07-06T04:26:22.450841Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#https://stackoverflow.com/questions/25962114/how-do-i-read-a-large-csv-file-with-pandas\n",
    "chunksize = 10e4\n",
    "print('chunksize=', chunksize)\n",
    "\n",
    "def process_big_csv(chunk, dest_file):\n",
    "    #---convert float64 to float32--------\n",
    "    float64_cols = chunk.select_dtypes(include=['float64']).columns.tolist()\n",
    "    chunk[float64_cols] = np.float32(chunk[float64_cols].values)\n",
    "    #---convert int64 to int32\n",
    "    int64_cols = chunk.select_dtypes(include=['int64']).columns.tolist()\n",
    "    chunk[int64_cols] = np.int32(chunk[int64_cols].values)\n",
    "    \n",
    "    #-- save to parquet file\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "    pq.write_table(table, dest_file, compression = 'GZIP')\n",
    "    \n",
    "    del table, chunk\n",
    "    gc.collect()\n",
    "\n",
    "train_file = '/kaggle/input/amex-default-prediction/test_data.csv'\n",
    "with pd.read_csv(train_file, chunksize=chunksize) as reader:\n",
    "    for i, chunk in enumerate(reader):\n",
    "        dest_file = f'{i+1}.parquet'\n",
    "        process_big_csv(chunk, dest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T05:24:22.560047Z",
     "iopub.status.busy": "2022-07-06T05:24:22.558210Z",
     "iopub.status.idle": "2022-07-06T05:24:23.202493Z",
     "shell.execute_reply": "2022-07-06T05:24:23.201116Z",
     "shell.execute_reply.started": "2022-07-06T05:24:22.559949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 669 ms, sys: 359 ms, total: 1.03 s\n",
      "Wall time: 634 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_parquet('114.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T05:24:25.062101Z",
     "iopub.status.busy": "2022-07-06T05:24:25.060557Z",
     "iopub.status.idle": "2022-07-06T05:24:25.094554Z",
     "shell.execute_reply": "2022-07-06T05:24:25.093356Z",
     "shell.execute_reply.started": "2022-07-06T05:24:25.062032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63762 entries, 11300000 to 11363761\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: float32(185), int32(1), object(4)\n",
      "memory usage: 47.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display files and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T05:25:41.057203Z",
     "iopub.status.busy": "2022-07-06T05:25:41.056710Z",
     "iopub.status.idle": "2022-07-06T05:25:41.067066Z",
     "shell.execute_reply": "2022-07-06T05:25:41.066171Z",
     "shell.execute_reply.started": "2022-07-06T05:25:41.057166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, ['65.parquet', '103.parquet'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = next(os.walk('.'))[2]\n",
    "parquet_files = []\n",
    "for file in files:\n",
    "    if '.parquet' in file:\n",
    "        parquet_files.append(file)\n",
    "\n",
    "len(parquet_files), parquet_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T05:22:43.035226Z",
     "iopub.status.busy": "2022-07-06T05:22:43.033902Z",
     "iopub.status.idle": "2022-07-06T05:22:43.862373Z",
     "shell.execute_reply": "2022-07-06T05:22:43.861273Z",
     "shell.execute_reply.started": "2022-07-06T05:22:43.035135Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9.0G\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:21 1.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:17 10.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:12 100.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:48 101.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:13 102.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:08 103.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:16 104.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:20 105.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:14 106.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:49 107.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:16 108.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:20 109.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:15 11.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:10 110.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:12 111.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:14 112.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:11 113.parquet\n",
      "-rw-r--r-- 1 root root 50M Jul  6 05:15 114.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:10 12.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:14 13.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:11 14.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:15 15.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:10 16.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:20 17.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:21 18.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:29 19.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:18 2.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:18 20.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:13 21.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:12 22.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:20 23.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:10 24.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:30 25.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:13 26.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:15 27.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:20 28.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:21 29.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:20 3.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:17 30.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:13 31.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:12 32.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:32 33.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:11 34.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:09 35.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:12 36.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:15 37.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:33 38.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:19 39.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:16 4.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:19 40.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:14 41.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:12 42.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:19 43.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:11 44.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:13 45.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:21 46.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:15 47.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:11 48.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:36 49.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:18 5.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:08 50.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:19 51.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:14 52.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:09 53.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:08 54.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:18 55.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:37 56.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:15 57.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:19 58.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:20 59.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:21 6.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:09 60.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:20 61.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:17 62.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:11 63.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:17 64.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:08 65.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:11 66.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:09 67.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:18 68.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:13 69.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:18 7.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:16 70.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:09 71.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:21 72.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:10 73.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:21 74.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:17 75.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:10 76.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:42 77.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:17 78.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:09 79.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:09 8.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:19 80.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:09 81.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:18 82.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:14 83.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:16 84.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:18 85.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:12 86.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:17 87.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:14 88.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:45 89.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:17 9.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:16 90.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:11 91.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:10 92.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:19 93.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:22 94.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:14 95.parquet\n",
      "-rw-r--r-- 1 root root 87M Jul  6 04:47 96.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:15 97.parquet\n",
      "-rw-r--r-- 1 root root 81M Jul  6 05:08 98.parquet\n",
      "-rw-r--r-- 1 root root 80M Jul  6 05:16 99.parquet\n",
      "---------- 1 root root 263 Jul  6 04:03 __notebook_source__.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"amex\\train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_parquet(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11363762 entries, 0 to 11363761\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: float32(185), int32(1), object(4)\n",
      "memory usage: 8.2+ GB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    table = pa.Table.from_pandas(train)\n",
    "    pq.write_table(table, r\"amex\\amex_train_20220706.parquet\", compression = 'GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_parquet( r\"amex\\amex_train_20220706.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
