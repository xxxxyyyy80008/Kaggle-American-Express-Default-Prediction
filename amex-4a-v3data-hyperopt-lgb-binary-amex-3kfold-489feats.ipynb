{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:55:29.759256Z",
     "iopub.status.busy": "2022-07-14T04:55:29.758659Z",
     "iopub.status.idle": "2022-07-14T04:55:29.778715Z",
     "shell.execute_reply": "2022-07-14T04:55:29.777010Z",
     "shell.execute_reply.started": "2022-07-14T04:55:29.759223Z"
    }
   },
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:55:29.781353Z",
     "iopub.status.busy": "2022-07-14T04:55:29.780400Z",
     "iopub.status.idle": "2022-07-14T04:55:29.826295Z",
     "shell.execute_reply": "2022-07-14T04:55:29.825207Z",
     "shell.execute_reply.started": "2022-07-14T04:55:29.781309Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:55:29.830030Z",
     "iopub.status.busy": "2022-07-14T04:55:29.829488Z",
     "iopub.status.idle": "2022-07-14T04:55:29.838606Z",
     "shell.execute_reply": "2022-07-14T04:55:29.837521Z",
     "shell.execute_reply.started": "2022-07-14T04:55:29.829996Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:55:29.840312Z",
     "iopub.status.busy": "2022-07-14T04:55:29.839826Z",
     "iopub.status.idle": "2022-07-14T04:55:29.854059Z",
     "shell.execute_reply": "2022-07-14T04:55:29.853084Z",
     "shell.execute_reply.started": "2022-07-14T04:55:29.840282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "random_seed=1234\n",
    "pl.seed_everything(random_seed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:55:29.858043Z",
     "iopub.status.busy": "2022-07-14T04:55:29.857693Z",
     "iopub.status.idle": "2022-07-14T04:55:53.179905Z",
     "shell.execute_reply": "2022-07-14T04:55:53.179047Z",
     "shell.execute_reply.started": "2022-07-14T04:55:29.858011Z"
    }
   },
   "source": [
    "%%time\n",
    "# train_file = r'/kaggle/input/amex-agg-data-rev2/agg_train_all_rev2_rev.parquet'\n",
    "train_file = r'amex/agg_v3/agg_train_all_small.parquet'\n",
    "\n",
    "df=pd.read_parquet(train_file, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "id_feats = ['customer_ID']\n",
    "#date_col =  'S_2'\n",
    "target_col = 'target'\n",
    "\n",
    "\n",
    "x_cols = ['B_10__last', 'B_10__mean', 'B_11__last', 'B_11__last__log', 'B_11__mean', 'B_11__mean__log', 'B_11__min', 'B_12__last', 'B_12__last__log', 'B_13__last', 'B_13__last__log', 'B_14__last', 'B_14__min', 'B_15__last', 'B_15__mean', 'B_16__last', 'B_16__max', 'B_16__min', 'B_17__last', 'B_17__max', 'B_17__mean', 'B_17__mean2std', 'B_17__min', 'B_18__last', 'B_18__last__log', 'B_18__mean__log', 'B_18__min', 'B_19__last', 'B_19__min', 'B_1__last', 'B_1__max', 'B_1__mean', 'B_1__min', 'B_20__last', 'B_20__max', 'B_20__mean', 'B_20__min', 'B_21__last', 'B_21__last__log', 'B_21__max', 'B_21__mean__log', 'B_22__last', 'B_22__last__log', 'B_22__max', 'B_22__mean', 'B_22__mean__log', 'B_22__min', 'B_23__last', 'B_23__last__log', 'B_23__mean', 'B_23__min', 'B_24__last', 'B_24__last__log', 'B_24__mean', 'B_24__mean__log', 'B_25__last', 'B_25__min', 'B_26__last', 'B_26__last__log', 'B_26__mean__log', 'B_27__last', 'B_27__last__log', 'B_28__last', 'B_29__last', 'B_29__last__log', 'B_2__last', 'B_2__mean', 'B_2__min', 'B_30=0.0', 'B_30=1.0', 'B_30=2.0', 'B_30__last', 'B_30__nunique', 'B_31__last', 'B_31__nunique', 'B_32__last', 'B_32__last__log', 'B_32__max', 'B_33__last', 'B_33__max', 'B_33__mean', 'B_33__min', 'B_36__last', 'B_36__last__log', 'B_37__last', 'B_37__max', 'B_37__mean', 'B_38=4.0', 'B_38=5.0', 'B_38=6.0', 'B_38=7.0', 'B_38__last', 'B_38__nunique', 'B_39__last', 'B_3__last', 'B_3__last__log', 'B_3__max', 'B_3__mean', 'B_3__mean__log', 'B_3__min', 'B_40__last', 'B_40__last__log', 'B_40__mean__log', 'B_40__min', 'B_41__last', 'B_41__last__log', 'B_42__last', 'B_42__last__log', 'B_4__last', 'B_4__last__log', 'B_4__max', 'B_4__mean2std', 'B_4__mean__log', 'B_5__last', 'B_5__last__log', 'B_5__mean', 'B_6__last', 'B_6__min', 'B_7__last', 'B_7__max', 'B_7__mean', 'B_7__min', 'B_8__last', 'B_8__max', 'B_8__mean', 'B_8__min', 'B_9__last', 'B_9__last__log', 'B_9__max', 'B_9__mean', 'B_9__mean__log', 'B_9__min', 'D_102__last', 'D_102__last__log', 'D_102__max', 'D_103__last', 'D_104__last', 'D_105__last', 'D_105__max', 'D_106__last', 'D_106__last__log', 'D_107__last', 'D_107__last__log', 'D_107__max', 'D_107__mean__log', 'D_108__last', 'D_108__last__log', 'D_109__last', 'D_109__last__log', 'D_109__mean', 'D_110__last', 'D_111__last', 'D_112__last', 'D_113__last', 'D_113__last__log', 'D_113__max', 'D_114=0.0', 'D_114__last', 'D_114__nunique', 'D_115__last', 'D_115__last__log', 'D_116__last', 'D_117__last', 'D_118__last', 'D_118__last__log', 'D_118__mean__log', 'D_119__last', 'D_119__last__log', 'D_120=1.0', 'D_120__last', 'D_120__nunique', 'D_121__last', 'D_122__last', 'D_122__max', 'D_122__min', 'D_123__last', 'D_123__last__log', 'D_123__mean', 'D_124__last', 'D_125__last', 'D_125__last__log', 'D_126__last', 'D_127__last', 'D_128__last', 'D_128__max', 'D_128__min', 'D_129__last', 'D_129__max', 'D_129__mean', 'D_130__last', 'D_130__max', 'D_131__last', 'D_131__last__log', 'D_131__max', 'D_131__min', 'D_132__last', 'D_132__mean2std', 'D_132__min', 'D_133__last', 'D_133__last__log', 'D_133__max', 'D_133__min', 'D_134__last', 'D_134__min', 'D_135__last', 'D_135__last__log', 'D_135__mean2std', 'D_136__last', 'D_136__last__log', 'D_137__last', 'D_137__last__log', 'D_138__last', 'D_138__last__log', 'D_139__last', 'D_139__mean', 'D_140__last', 'D_140__last__log', 'D_140__max', 'D_141__last', 'D_142__last', 'D_143__last', 'D_144__last', 'D_145__last', 'D_39__last', 'D_39__last__log', 'D_39__max', 'D_39__mean', 'D_41__last', 'D_41__last__log', 'D_41__max', 'D_41__mean', 'D_41__min', 'D_42__last', 'D_42__max', 'D_42__mean', 'D_42__mean2std', 'D_42__min', 'D_43__last', 'D_43__last__log', 'D_43__max', 'D_43__mean', 'D_43__mean__log', 'D_43__min', 'D_44__last', 'D_44__last__log', 'D_44__max', 'D_44__mean', 'D_44__mean__log', 'D_44__min', 'D_45__last', 'D_45__last__log', 'D_45__max', 'D_45__mean', 'D_45__mean__log', 'D_45__min', 'D_46__last', 'D_46__mean', 'D_46__mean2std', 'D_46__min', 'D_47__last', 'D_48__last', 'D_48__max', 'D_48__mean', 'D_48__min', 'D_49__last', 'D_49__last__log', 'D_49__mean2std', 'D_50__last', 'D_51__last', 'D_51__last__log', 'D_51__mean', 'D_52__last', 'D_52__max', 'D_52__mean', 'D_52__mean2std', 'D_52__min', 'D_53__last', 'D_53__max', 'D_53__mean2std', 'D_53__min', 'D_54__last', 'D_55__last', 'D_55__min', 'D_56__last', 'D_56__min', 'D_58__last', 'D_58__min', 'D_59__last', 'D_59__max', 'D_60__last', 'D_60__last__log', 'D_60__max', 'D_61__last', 'D_61__max', 'D_61__mean', 'D_61__min', 'D_62__last', 'D_62__max', 'D_62__mean', 'D_62__min', 'D_64=U', 'D_65__last', 'D_65__max', 'D_65__mean', 'D_68=1.0', 'D_68__last', 'D_69__last', 'D_69__min', 'D_70__last', 'D_70__max', 'D_70__mean2std', 'D_70__min', 'D_71__last', 'D_72__last', 'D_72__max', 'D_72__min', 'D_73__last', 'D_74__last', 'D_74__max', 'D_74__mean', 'D_75__last', 'D_75__max', 'D_75__mean', 'D_76__last', 'D_77__last', 'D_77__max', 'D_77__mean', 'D_77__min', 'D_78__last', 'D_78__max', 'D_78__mean', 'D_79__last', 'D_79__max', 'D_80__last', 'D_81__last', 'D_81__max', 'D_81__mean', 'D_82__last', 'D_83__last', 'D_84__last', 'D_84__max', 'D_84__mean', 'D_86__last', 'D_87__last', 'D_88__last', 'D_89__last', 'D_89__max', 'D_89__mean', 'D_91__last', 'D_92__last', 'D_93__last', 'D_94__last', 'D_96__last', 'P_2__last', 'P_2__max', 'P_2__mean', 'P_2__min', 'P_3__last', 'P_3__max', 'P_3__mean', 'P_3__min', 'P_4__last', 'P_4__max', 'P_4__mean', 'P_4__min', 'R_10__last', 'R_10__max', 'R_10__mean', 'R_11__last', 'R_11__max', 'R_11__mean', 'R_12__last', 'R_12__max', 'R_12__mean', 'R_13__last', 'R_13__max', 'R_13__mean', 'R_13__mean2std', 'R_14__last', 'R_15__last', 'R_15__max', 'R_15__mean', 'R_16__last', 'R_16__max', 'R_16__mean', 'R_17__last', 'R_17__max', 'R_18__last', 'R_18__max', 'R_19__last', 'R_19__min', 'R_1__last', 'R_1__max', 'R_1__mean', 'R_1__min', 'R_20__last', 'R_20__max', 'R_20__mean', 'R_21__last', 'R_22__last', 'R_22__max', 'R_23__last', 'R_24__last', 'R_24__max', 'R_24__mean', 'R_25__last', 'R_25__max', 'R_26__last', 'R_26__mean2std', 'R_27__last', 'R_27__max', 'R_27__mean', 'R_27__min', 'R_28__last', 'R_28__last__log', 'R_2__last', 'R_2__max', 'R_2__mean', 'R_2__min', 'R_3__last', 'R_3__max', 'R_3__mean', 'R_3__min', 'R_4__last', 'R_4__max', 'R_4__mean', 'R_4__min', 'R_5__last', 'R_5__max', 'R_5__mean', 'R_6__last', 'R_6__max', 'R_6__mean', 'R_7__last', 'R_7__max', 'R_7__mean', 'R_8__last', 'R_8__max', 'R_8__mean', 'R_9__last', 'R_9__mean2std', 'S_11__last', 'S_11__mean', 'S_12__last', 'S_12__max', 'S_13__last', 'S_15__last', 'S_15__max', 'S_15__mean', 'S_16__last', 'S_17__last', 'S_17__min', 'S_18__last', 'S_19__last', 'S_20__last', 'S_20__max', 'S_22__last', 'S_22__max', 'S_22__mean', 'S_23__last', 'S_23__max', 'S_23__mean', 'S_24__last', 'S_25__last', 'S_25__mean', 'S_25__min', 'S_26__last', 'S_26__last__log', 'S_27__last', 'S_27__mean2std', 'S_3__last', 'S_3__max', 'S_3__mean', 'S_3__min', 'S_5__last', 'S_5__last__log', 'S_5__mean__log', 'S_6__last', 'S_6__max', 'S_6__min', 'S_7__last', 'S_7__max', 'S_7__mean', 'S_7__min', 'S_8__last', 'S_8__mean', 'S_8__min', 'S_9__last', 'S_9__max']\n",
    "\n",
    "\n",
    "len(x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train_file = r'/kaggle/input/amex-agg-data-rev2/agg_train_all_rev2_rev.parquet'\n",
    "train_file = r'amex/agg_v3/agg_train_all_small.parquet'\n",
    "df = pd.read_parquet(train_file, columns=id_feats + [target_col] + x_cols, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperopt parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:30.244072Z",
     "iopub.status.busy": "2022-07-14T04:56:30.242862Z",
     "iopub.status.idle": "2022-07-14T04:56:30.252666Z",
     "shell.execute_reply": "2022-07-14T04:56:30.251727Z",
     "shell.execute_reply.started": "2022-07-14T04:56:30.244012Z"
    }
   },
   "outputs": [],
   "source": [
    "learn_rates = np.concatenate((\n",
    "#                            np.arange(0.00001, 0.0001, 0.00001),  \n",
    "                           np.arange(0.0001, 0.001, 0.0001), \n",
    "                           np.arange(0.001, 0.01, 0.001), \n",
    "                           np.arange(0.01, 0.1, 0.01)\n",
    "                          ), \n",
    "                          axis=0)\n",
    "\n",
    "\n",
    "len(learn_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:30.645644Z",
     "iopub.status.busy": "2022-07-14T04:56:30.645011Z",
     "iopub.status.idle": "2022-07-14T04:56:30.889333Z",
     "shell.execute_reply": "2022-07-14T04:56:30.888117Z",
     "shell.execute_reply.started": "2022-07-14T04:56:30.645602Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "#https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html?highlight=classifier#lightgbm.LGBMClassifier\n",
    "\n",
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "space  = { \n",
    "    \n",
    "                 'n_estimators': hp.choice('n_estimators', range(500, 2000, 50)),#num_boost_round\n",
    "                 'boosting_type':hp.choice('boosting_type', ['gbdt']),#boosting\n",
    "                 'objective':hp.choice('objective', ['binary'] ),\n",
    "                 'metric':hp.choice('metric', ['auc']),\n",
    "                 'learning_rate':  hp.choice('learning_rate', learn_rates), \n",
    "                 'colsample_bytree': hp.choice('colsample_bytree', np.round(np.arange(0.1, 0.86, 0.05),3)), #feature_fraction\n",
    "                 'max_depth': hp.choice('max_depth', range(7, 36, 1)), #int type\n",
    "                 'min_child_samples': hp.choice('min_child_samples',range(100, 5001, 50)), #min_data_in_leaf\n",
    "                 'reg_alpha':hp.choice('reg_alpha', [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10,15]),#lambda_l1\n",
    "                 'reg_lambda':hp.choice('reg_lambda', [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10,15]),#lambda_l2\n",
    "                 'max_bin':hp.choice('max_bin', range(500, 10000, 50)),\n",
    "                 'min_data_in_bin':hp.choice('min_data_in_bin', range(500, 9000, 50)),\n",
    "                 'subsample':hp.choice('subsample', np.round(np.arange(0.1, 0.96, 0.05),3)), #bagging_fraction\n",
    "                 'subsample_freq':hp.choice('subsample_freq', range(1, 100, 2)),#bagging_freq\n",
    "                 #max number of leaves in one tree. 1 < num_leaves <= 131072. classes< num_leaves< 2^max_depth  \n",
    "                 'num_leaves':hp.choice('num_leaves', range(31, 300, 5)),#max_leaves. \n",
    "                 'random_state':hp.choice('random_state', [1234]),\n",
    "                 'n_jobs':hp.choice('n_jobs', [4]),#nthread\n",
    "                 #'min_split_gain':hp.choice('min_split_gain', [0.0]), #min_gain_to_split\n",
    "                 #'min_child_weight':hp.choice('min_child_weight', [0.001]),   #min_sum_hessian_in_leaf\n",
    "                 #'subsample_for_bin':hp.choice('subsample_for_bin', [200000]),   #bin_construct_sample_cnt \n",
    "                 #'importance_type':hp.choice('importance_type', ['split']),   \n",
    "    \n",
    "                  }\n",
    "                  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:31.888340Z",
     "iopub.status.busy": "2022-07-14T04:56:31.887937Z",
     "iopub.status.idle": "2022-07-14T04:56:32.811837Z",
     "shell.execute_reply": "2022-07-14T04:56:32.810649Z",
     "shell.execute_reply.started": "2022-07-14T04:56:31.888309Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def train_trees(X_train, y_train, num_round=100, params={} ):\n",
    "    \n",
    "    params['verbosity'] = -1\n",
    "    dtrain = lgb.Dataset(X_train, y_train)\n",
    "    \n",
    "    tree_model = lgb.train(params,\n",
    "                dtrain,\n",
    "                num_boost_round=num_round)\n",
    "    \n",
    "    \n",
    "    del dtrain\n",
    "    gc.collect()\n",
    "\n",
    "    return tree_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:32.814342Z",
     "iopub.status.busy": "2022-07-14T04:56:32.813616Z",
     "iopub.status.idle": "2022-07-14T04:56:32.824557Z",
     "shell.execute_reply": "2022-07-14T04:56:32.823303Z",
     "shell.execute_reply.started": "2022-07-14T04:56:32.814302Z"
    }
   },
   "outputs": [],
   "source": [
    "# @yunchonggan's fast metric implementation\n",
    "# From https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "# https://www.kaggle.com/code/ambrosm/amex-lightgbm-quickstart\n",
    "def amex_metric(y_true: np.array, y_pred: np.array) -> float:\n",
    "\n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T01:53:30.259601Z",
     "iopub.status.busy": "2022-07-14T01:53:30.25927Z",
     "iopub.status.idle": "2022-07-14T01:53:30.271963Z",
     "shell.execute_reply": "2022-07-14T01:53:30.270859Z",
     "shell.execute_reply.started": "2022-07-14T01:53:30.259572Z"
    }
   },
   "source": [
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trues = [np.random.randint(2) for i in range(100)]\n",
    "\n",
    "\n",
    "amex_metric(pd.DataFrame(data={'target': trues}), \n",
    "            pd.DataFrame(data={'prediction': np.random.rand(100)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:33.832975Z",
     "iopub.status.busy": "2022-07-14T04:56:33.832535Z",
     "iopub.status.idle": "2022-07-14T04:56:34.057519Z",
     "shell.execute_reply": "2022-07-14T04:56:34.056089Z",
     "shell.execute_reply.started": "2022-07-14T04:56:33.832931Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[x_cols]\n",
    "y = df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:34.096438Z",
     "iopub.status.busy": "2022-07-14T04:56:34.095611Z",
     "iopub.status.idle": "2022-07-14T04:56:34.103130Z",
     "shell.execute_reply": "2022-07-14T04:56:34.102270Z",
     "shell.execute_reply.started": "2022-07-14T04:56:34.096396Z"
    }
   },
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:34.977361Z",
     "iopub.status.busy": "2022-07-14T04:56:34.976765Z",
     "iopub.status.idle": "2022-07-14T04:56:35.037781Z",
     "shell.execute_reply": "2022-07-14T04:56:35.036381Z",
     "shell.execute_reply.started": "2022-07-14T04:56:34.977325Z"
    }
   },
   "outputs": [],
   "source": [
    "print(skf)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index), len(test_index)/(len(test_index)+len(train_index)))\n",
    "    print(y.iloc[test_index]['target'].value_counts()/len(test_index)) \n",
    "    print(y.iloc[train_index]['target'].value_counts()/len(train_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:35.305240Z",
     "iopub.status.busy": "2022-07-14T04:56:35.304856Z",
     "iopub.status.idle": "2022-07-14T04:56:35.311399Z",
     "shell.execute_reply": "2022-07-14T04:56:35.310627Z",
     "shell.execute_reply.started": "2022-07-14T04:56:35.305208Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_index), len(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'amex/agg_v3/lgb-hyperopt-v3data-3kfold-489feats.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:35.633498Z",
     "iopub.status.busy": "2022-07-14T04:56:35.632803Z",
     "iopub.status.idle": "2022-07-14T04:56:35.643346Z",
     "shell.execute_reply": "2022-07-14T04:56:35.642086Z",
     "shell.execute_reply.started": "2022-07-14T04:56:35.633463Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_dict = []\n",
    "\n",
    "\n",
    "def score(params):\n",
    "#     print(params)\n",
    "    num_boost_round = params['n_estimators']\n",
    "    params_ = copy.deepcopy(params)\n",
    "    del params_['n_estimators']\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        #----start: data prep-------------------------------------\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        #----end: data prep-------------------------------------\n",
    "        \n",
    "        #-----start: train trees---------------------------------------\n",
    "        tree_model = train_trees(X_train, y_train['target'].values,\n",
    "                                 num_round=num_boost_round, \n",
    "                                 params=params_\n",
    "                                )\n",
    "\n",
    "        y_preds = tree_model.predict(X_test, num_iteration=tree_model.best_iteration)\n",
    "#         loss = roc_auc_score(y_test, y_preds)\n",
    "#         loss = f1_score(y_test['target'].values, (y_preds>=0.9).astype(int))\n",
    "#         loss = amex_metric(y_test, \n",
    "#                            pd.DataFrame(data={'prediction': y_preds}))\n",
    "        loss = amex_metric(y_test['target'].values, y_preds)\n",
    "        losses.append(loss)\n",
    "        #-----end: train trees---------------------------------------\n",
    "        \n",
    "    loss = np.mean(losses)\n",
    "#     print(loss)\n",
    "    loss_dict.append({'params': params, 'losses': losses, 'mean_loss': loss})\n",
    "    \n",
    "    if len(loss_dict)%10==0:\n",
    "        pd.DataFrame(data=loss_dict).to_excel(log_file, index=False)\n",
    "    return {'loss': -loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:36.562585Z",
     "iopub.status.busy": "2022-07-14T04:56:36.561382Z",
     "iopub.status.idle": "2022-07-14T04:56:36.570600Z",
     "shell.execute_reply": "2022-07-14T04:56:36.569371Z",
     "shell.execute_reply.started": "2022-07-14T04:56:36.562462Z"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, anneal, rand\n",
    "from functools import partial\n",
    "def optimize(space, evals, cores, trials, optimizer=tpe.suggest, random_state=1234, n_startup_jobs=10):\n",
    "    algo = partial(optimizer, n_startup_jobs=n_startup_jobs)\n",
    "    best = fmin(score, space, algo=algo, max_evals=evals, trials = trials)\n",
    "    print(best)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:37.203450Z",
     "iopub.status.busy": "2022-07-14T04:56:37.202290Z",
     "iopub.status.idle": "2022-07-14T04:56:37.209376Z",
     "shell.execute_reply": "2022-07-14T04:56:37.208282Z",
     "shell.execute_reply.started": "2022-07-14T04:56:37.203401Z"
    }
   },
   "outputs": [],
   "source": [
    "cores = 4\n",
    "n=500\n",
    "verbose = False\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T04:56:37.566102Z",
     "iopub.status.busy": "2022-07-14T04:56:37.565207Z"
    }
   },
   "outputs": [],
   "source": [
    "best_param = optimize(space,\n",
    "                      evals = n,\n",
    "                      optimizer=tpe.suggest,\n",
    "                      cores = cores,\n",
    "                      trials = trials, random_state=1234, \n",
    "                      n_startup_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=loss_dict).to_excel(log_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
